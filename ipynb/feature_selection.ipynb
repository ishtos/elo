{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from datetime import datetime, date\n",
    "from collections import defaultdict\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "import eli5\n",
    "from eli5.lightgbm import explain_weights_lightgbm\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "import shap \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join('..', 'remove_outlier_data')\n",
    "\n",
    "params = {\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 30, \n",
    "    'objective':'regression',\n",
    "    'max_depth': -1,\n",
    "    'learning_rate': 0.01,\n",
    "    'boosting': 'gbdt',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_seed': 11,\n",
    "    'metric': 'rmse',\n",
    "    'lambda_l1': 0.1,\n",
    "    'verbosity': -1,\n",
    "    'nthread': cpu_count(),\n",
    "    'random_state': 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "features +=  [f'f10{i}.pkl' for i in (2, 3)]\n",
    "features += [f'f11{i}_{j}.pkl' for i in (1, 2) \n",
    "                               for j in ('Y', 'N')]\n",
    "features += [f'f12{i}.pkl' for i in (1, 2)]\n",
    "\n",
    "\n",
    "features += [f'f20{i}.pkl' for i in (2, 3)]\n",
    "features += [f'f21{i}_{j}.pkl' for i in (1, 2)\n",
    "                               for j in ('Y', 'N')]\n",
    "\n",
    "features += [f'f40{i}.pkl' for i in (2, 3)]\n",
    "features += [f'f41{i}_{j}.pkl' for i in (1, 2)\n",
    "                               for j in ('Y', 'N')]\n",
    "features += [f'f42{i}.pkl' for i in (1, 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "\n",
    "for f in tqdm(features):\n",
    "    t = pd.read_pickle(os.path.join('..', 'remove_outlier_feature', f))\n",
    "    train = pd.merge(train, t, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train.columns.values\n",
    "for f in [\n",
    "    'new_purchase_date_max', 'new_purchase_date_min',\n",
    "    'hist_purchase_date_max', 'hist_purchase_date_min', \n",
    "    'N_hist_auth_purchase_date_max', 'N_hist_auth_purchase_date_min',\n",
    "    'Y_hist_auth_purchase_date_max', 'Y_hist_auth_purchase_date_min', \n",
    "    'Y_new_auth_purchase_date_max', 'Y_new_auth_purchase_date_min', \n",
    "    'N_new_auth_purchase_date_max', 'N_new_auth_purchase_date_min',\n",
    "    'Y_new_auth_purchase_date_max_x', 'Y_new_auth_purchase_date_min_x', \n",
    "    'N_new_auth_purchase_date_max_x', 'N_new_auth_purchase_date_min_x', \n",
    "    'Y_new_auth_purchase_date_max_y', 'Y_new_auth_purchase_date_min_y', \n",
    "    'N_new_auth_purchase_date_max_y', 'N_new_auth_purchase_date_min_y'\n",
    "]:\n",
    "    if f in cols:\n",
    "        train[f] = train[f].astype(np.int64) * 1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']\n",
    "\n",
    "col_not_to_use = ['first_active_month', 'card_id', 'target']\n",
    "col_to_use = [c for c in train.columns if c not in col_not_to_use]\n",
    "\n",
    "train = train[col_to_use]\n",
    "train['feature_3'] = train['feature_3'].astype(int)\n",
    "\n",
    "categorical_features = ['feature_1', 'feature_2', 'feature_3']\n",
    "\n",
    "for col in categorical_features:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train[col].values.astype('str')))\n",
    "    train[col] = lbl.transform(list(train[col].values.astype('str')))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=6)\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "    dtrain = lgb.Dataset(X.iloc[train_index], label=y.iloc[train_index])\n",
    "    dvalid = lgb.Dataset(X.iloc[valid_index], label=y.iloc[valid_index])\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        20000,          \n",
    "        valid_sets=[dtrain, dvalid],\n",
    "        verbose_eval=200,\n",
    "        early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "shap_values = explainer.shap_values(X.iloc[valid_index])\n",
    "shap.summary_plot(shap_values, X.iloc[valid_index], max_display=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# libFFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dot, Reshape, Add, Subtract\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "\n",
    "#==============================================================================\n",
    "PREF = 'f501'\n",
    "\n",
    "KEY = 'card_id'\n",
    "\n",
    "# =============================================================================\n",
    "# def\n",
    "# =============================================================================\n",
    "\n",
    "def get_embed(x_input, x_size, k_latent):\n",
    "    if x_size > 0:  \n",
    "        embed = Embedding(x_size, k_latent, input_length=1,\n",
    "                          embeddings_regularizer=l2(embedding_reg))(x_input)\n",
    "        embed = Flatten()(embed)\n",
    "    else:\n",
    "        embed = Dense(k_latent, kernel_regularizer=l2(embedding_reg))(x_input)\n",
    "    return embed\n",
    "\n",
    "\n",
    "def build_model_1(X, fsize):\n",
    "    dim_input = len(fsize)\n",
    "\n",
    "    input_x = [Input(shape=(1,)) for i in range(dim_input)]\n",
    "\n",
    "    biases = [get_embed(x, size, 1) for (x, size) in zip(input_x, fsize)]\n",
    "\n",
    "    factors = [get_embed(x, size, k_latent)\n",
    "               for (x, size) in zip(input_x, fsize)]\n",
    "\n",
    "    s = Add()(factors)\n",
    "\n",
    "    diffs = [Subtract()([s, x]) for x in factors]\n",
    "\n",
    "    dots = [Dot(axes=1)([d, x]) for d, x in zip(diffs, factors)]\n",
    "\n",
    "    x = Concatenate()(biases + dots)\n",
    "    x = BatchNormalization()(x)\n",
    "    output = Dense(1, activation='relu', kernel_regularizer=l2(kernel_reg))(x)\n",
    "    model = Model(inputs=input_x, outputs=[output])\n",
    "    opt = Adam(clipnorm=0.5)\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    output_f = factors + biases\n",
    "    model_features = Model(inputs=input_x, outputs=output_f)\n",
    "\n",
    "    return model, model_features\n",
    "\n",
    "# =============================================================================\n",
    "# main\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "train = pd.read_csv('../remove_outlier_data/train.csv')\n",
    "test = pd.read_csv('../remove_outlier_data/test.csv')\n",
    "\n",
    "\n",
    "features = []\n",
    "features +=  [f'f10{i}.pkl' for i in (2, 3)]\n",
    "features += [f'f11{i}_{j}.pkl' for i in (1, 2) \n",
    "                               for j in ('Y', 'N')]\n",
    "features += [f'f12{i}.pkl' for i in (1, 2)]\n",
    "\n",
    "\n",
    "features += [f'f20{i}.pkl' for i in (2, 3)]\n",
    "features += [f'f21{i}_{j}.pkl' for i in (1, 2)\n",
    "                               for j in ('Y', 'N')]\n",
    "\n",
    "features += [f'f40{i}.pkl' for i in (2, 3)]\n",
    "features += [f'f41{i}_{j}.pkl' for i in (1, 2)\n",
    "                               for j in ('Y', 'N')]\n",
    "features += [f'f42{i}.pkl' for i in (1, 2)]\n",
    "\n",
    "for f in tqdm(features):\n",
    "    t = pd.read_pickle(os.path.join('..', 'remove_outlier_feature', f))\n",
    "    train = pd.merge(train, t, on='card_id', how='left')\n",
    "    test = pd.merge(test, t, on='card_id', how='left')\n",
    "\n",
    "df = pd.concat([train, test], axis=0, sort=False)\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c, d in zip(df.columns, df.dtypes):\n",
    "    if ('int' in str(d)):\n",
    "        print('int:', c)\n",
    "#     if ('float' in str(d)):\n",
    "#         print('float:', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 18\n",
    "np.random.seed(SEED)\n",
    "\n",
    "features = [\n",
    "#     'feature_1', 'feature_2', 'feature_3', \n",
    "#     'days_feature_1', 'days_feature_2', 'days_feature_3',\n",
    "    'hist_category_1_sum', 'hist_category_2_nunique', 'hist_category_3_nunique',\n",
    "    'Y_new_auth_category_1_sum', 'Y_new_auth_category_2_nunique', 'Y_new_auth_category_3_nunique',\n",
    "    'union_category_1_sum', 'union_category_2_nunique', 'union_category_3_nunique',\n",
    "]\n",
    "\n",
    "fsize = [int(df[f].max()) + 1 for f in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.groupby(features)['card_id'].count()\n",
    "\n",
    "X = X.unstack().fillna(0)\n",
    "X = X.stack().astype('float32')\n",
    "X = np.log1p(X).reset_index()\n",
    "X.columns = features + ['num']\n",
    "\n",
    "X_train = [X[f].values for f in features]\n",
    "y_train = (X[['num']].values).astype('float32')\n",
    "\n",
    "k_latent = 3\n",
    "embedding_reg = 0.0002\n",
    "kernel_reg = 0.1\n",
    "\n",
    "model, model_features = build_model_1(X_train, fsize)\n",
    "\n",
    "n_epochs = 20\n",
    "\n",
    "batch_size = 2 ** 17\n",
    "model, model_features = build_model_1(X_train, fsize)\n",
    "earlystopper = EarlyStopping(patience=0, verbose=50)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,  y_train,\n",
    "    epochs=n_epochs, batch_size=batch_size, verbose=1, shuffle=True,\n",
    "    validation_data=(X_train, y_train),\n",
    "    callbacks=[earlystopper],\n",
    ")\n",
    "# model.save('weights/{}_weights.h5'.format(str(date.today()).replace('-', '')))\n",
    "\n",
    "X_pred = model_features.predict(X_train, batch_size=batch_size)\n",
    "\n",
    "factors = X_pred[:len(features)]\n",
    "\n",
    "biases = X_pred[len(features):2*len(features)]\n",
    "\n",
    "for f, X_p in zip(features, factors):\n",
    "    for i in range(k_latent):\n",
    "        X['%s_fm_factor_%d' % (f, i)] = X_p[:, i]\n",
    "\n",
    "# for f, X_p in zip(features, biases):\n",
    "#     for i in range(k_latent):\n",
    "#         X['%s_fm_bias' % (f, i)] = X_p[:, 0i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df[['card_id', 'feature_1', 'feature_2', 'feature_3']], X, on=['feature_1', 'feature_2', 'feature_3'], how='left')\n",
    "df = df.drop(features, axis=1)\n",
    "df.to_pickle(f'../remove_outlier_feature/{PREF}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from keras.layers import Input, Embedding, Dense,Flatten, merge,Activation\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2 as l2_reg\n",
    "from keras import initializers\n",
    "import itertools\n",
    "\n",
    "\n",
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.ceil(size/float(batch_size)))\n",
    "    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n",
    "\n",
    "\n",
    "def batch_generator(X,y,batch_size=128,shuffle=True):\n",
    "    sample_size = X[0].shape[0]\n",
    "    index_array = np.arange(sample_size)\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(index_array)\n",
    "        batches = make_batches(sample_size, batch_size)\n",
    "        for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "            batch_ids = index_array[batch_start:batch_end]\n",
    "            X_batch = [X[i][batch_ids] for i in range(len(X))]\n",
    "            y_batch = y[batch_ids]\n",
    "            yield X_batch,y_batch\n",
    "\n",
    "\n",
    "def test_batch_generator(X,y,batch_size=128):\n",
    "    sample_size = X[0].shape[0]\n",
    "    index_array = np.arange(sample_size)\n",
    "    batches = make_batches(sample_size, batch_size)\n",
    "    for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "        batch_ids = index_array[batch_start:batch_end]\n",
    "        X_batch = [X[i][batch_ids] for i in range(len(X))]\n",
    "        y_batch = y[batch_ids]\n",
    "        yield X_batch,y_batch\n",
    "\n",
    "\n",
    "def predict_batch(model,X_t,batch_size=128):\n",
    "    outcome = []\n",
    "    for X_batch,y_batch in test_batch_generator(X_t,np.zeros(X_t[0].shape[0]),batch_size=batch_size):\n",
    "        outcome.append(model.predict(X_batch,batch_size=batch_size))\n",
    "    outcome = np.concatenate(outcome).ravel()\n",
    "    return outcome\n",
    "\n",
    "\n",
    "\n",
    "def build_model(max_features,K=8,solver='adam',l2=0.0,l2_fm = 0.0):\n",
    "\n",
    "    inputs = []\n",
    "    flatten_layers=[]\n",
    "    columns = range(len(max_features))\n",
    "    for c in columns:\n",
    "        inputs_c = Input(shape=(1,), dtype='int32',name = 'input_%s'%c)\n",
    "        num_c = max_features[c]\n",
    "\n",
    "        embed_c = Embedding(\n",
    "                        num_c,\n",
    "                        K,\n",
    "                        input_length=1,\n",
    "                        name = 'embed_%s'%c,\n",
    "                        W_regularizer=l2_reg(l2_fm)\n",
    "                        )(inputs_c)\n",
    "\n",
    "        flatten_c = Flatten()(embed_c)\n",
    "        inputs.append(inputs_c)\n",
    "        flatten_layers.append(flatten_c)\n",
    "\n",
    "    fm_layers = []\n",
    "    for emb1,emb2 in itertools.combinations(flatten_layers, 2):\n",
    "        dot_layer = merge([emb1,emb2],mode='dot',dot_axes=1)\n",
    "        fm_layers.append(dot_layer)\n",
    "\n",
    "    for c in columns:\n",
    "        num_c = max_features[c]\n",
    "        embed_c = Embedding(\n",
    "                        num_c,\n",
    "                        1,\n",
    "                        input_length=1,\n",
    "                        name = 'linear_%s'%c,\n",
    "                        W_regularizer=l2_reg(l2)\n",
    "                        )(inputs[c])\n",
    "\n",
    "        flatten_c = Flatten()(embed_c)\n",
    "\n",
    "        fm_layers.append(flatten_c)\n",
    "        \n",
    "        \n",
    "    flatten = merge(fm_layers,mode='sum')\n",
    "    outputs = Activation('sigmoid',name='outputs')(flatten)\n",
    "    \n",
    "    model = Model(input=inputs, output=outputs)\n",
    "\n",
    "    model.compile(\n",
    "                optimizer=solver,\n",
    "                loss= 'binary_crossentropy'\n",
    "              )\n",
    "\n",
    "    return model\n",
    "\n",
    "class KerasFM(BaseEstimator):\n",
    "    def __init__(self,max_features=[],K=8,solver='adam',l2=0.0,l2_fm = 0.0):\n",
    "        self.model = build_model(max_features,K,solver,l2=l2,l2_fm = l2_fm)\n",
    "\n",
    "    def fit(self,X,y,batch_size=128,nb_epoch=10,shuffle=True,verbose=1,validation_data=None):\n",
    "        self.model.fit(X,y,batch_size=batch_size,nb_epoch=nb_epoch,shuffle=shuffle,verbose=verbose,validation_data=None)\n",
    "\n",
    "    def fit_generator(self,X,y,batch_size=128,nb_epoch=10,shuffle=True,verbose=1,validation_data=None,callbacks=None):\n",
    "        tr_gen = batch_generator(X,y,batch_size=batch_size,shuffle=shuffle)\n",
    "        if validation_data:\n",
    "            X_test,y_test = validation_data\n",
    "            te_gen = batch_generator(X_test,y_test,batch_size=batch_size,shuffle=False)\n",
    "            nb_val_samples = X_test[-1].shape[0]\n",
    "        else:\n",
    "            te_gen = None\n",
    "            nb_val_samples = None\n",
    "\n",
    "        self.model.fit_generator(\n",
    "                tr_gen, \n",
    "                samples_per_epoch=X[-1].shape[0], \n",
    "                nb_epoch=nb_epoch, \n",
    "                verbose=verbose, \n",
    "                callbacks=callbacks, \n",
    "                validation_data=te_gen, \n",
    "                nb_val_samples=nb_val_samples, \n",
    "                max_q_size=10\n",
    "                )\n",
    "\n",
    "    def predict(self,X,batch_size=128):\n",
    "        y_preds = predict_batch(self.model,X,batch_size=batch_size)\n",
    "        return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join('..', 'remove_outlier_data')\n",
    "\n",
    "KEY = 'card_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "features +=  [f'f10{i}.pkl' for i in (2, 3)]\n",
    "# features += [f'f11{i}_{j}.pkl' for i in (1, 2) \n",
    "#                                for j in ('Y', 'N')]\n",
    "# features += [f'f12{i}.pkl' for i in (1, 2)]\n",
    "\n",
    "\n",
    "features += [f'f20{i}.pkl' for i in (2, 3)]\n",
    "# features += [f'f21{i}_{j}.pkl' for i in (1, 2)\n",
    "#                                for j in ('Y', 'N')]\n",
    "\n",
    "features += [f'f40{i}.pkl' for i in (2, 3)]\n",
    "# features += [f'f41{i}_{j}.pkl' for i in (1, 2)\n",
    "#                                for j in ('Y', 'N')]\n",
    "# features += [f'f42{i}.pkl' for i in (1, 2)]\n",
    "\n",
    "\n",
    "# features = os.listdir('../remove_outlier_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:03<00:00,  1.74it/s]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(PATH, 'test.csv'))\n",
    "\n",
    "for f in tqdm(features):\n",
    "    # print(f'Merge: {f}', end=' ')\n",
    "    t = pd.read_pickle(os.path.join('..', 'remove_outlier_feature', f))\n",
    "    train = pd.merge(train, t, on=KEY, how='left')\n",
    "    test = pd.merge(test, t, on=KEY, how='left')\n",
    "    # print('Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train.columns.values\n",
    "for f in [\n",
    "    'new_purchase_date_max', 'new_purchase_date_min',\n",
    "    'hist_purchase_date_max', 'hist_purchase_date_min', \n",
    "    'N_hist_auth_purchase_date_max', 'N_hist_auth_purchase_date_min',\n",
    "    'Y_hist_auth_purchase_date_max', 'Y_hist_auth_purchase_date_min', \n",
    "    'Y_new_auth_purchase_date_max', 'Y_new_auth_purchase_date_min', \n",
    "    'N_new_auth_purchase_date_max', 'N_new_auth_purchase_date_min',\n",
    "    'Y_new_auth_purchase_date_max_x', 'Y_new_auth_purchase_date_min_x', \n",
    "    'N_new_auth_purchase_date_max_x', 'N_new_auth_purchase_date_min_x', \n",
    "    'Y_new_auth_purchase_date_max_y', 'Y_new_auth_purchase_date_min_y', \n",
    "    'N_new_auth_purchase_date_max_y', 'N_new_auth_purchase_date_min_y'\n",
    "]:\n",
    "    if f in cols:\n",
    "        train[f] = train[f].astype(np.int64) * 1e-9\n",
    "        test[f] = test[f].astype(np.int64) * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_cols = [c for d, c in zip(train.dtypes, train.columns) if str(d).startswith('int')]\n",
    "len(int_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ffm_columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feature_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feature_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feature_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>first_active_month_year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>first_active_month_weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>first_active_month_month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>first_active_month_weekofyear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>first_active_month_quarter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>first_active_month_is_month_start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>elapsed_time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>days_feature_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>days_feature_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>days_feature_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hist_transactions_count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hist_category_1_sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hist_category_2_nunique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hist_category_3_nunique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hist_merchant_id_nunique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hist_state_id_nunique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hist_subsector_id_nunique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hist_city_id_nunique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hist_merchant_category_id_nunique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hist_installments_nunique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hist_purchase_month_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>hist_purchase_month_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hist_month_diff_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>hist_month_diff_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hist_purchase_date_diff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>hist_purchase_date_uptonow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>union_transactions_count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>union_category_1_sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>union_category_2_nunique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>union_category_3_nunique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>union_merchant_id_nunique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>union_state_id_nunique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>union_subsector_id_nunique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>union_city_id_nunique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>union_merchant_category_id_nunique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>union_installments_nunique</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           ffm_columns\n",
       "0                            feature_1\n",
       "1                            feature_2\n",
       "2                            feature_3\n",
       "3              first_active_month_year\n",
       "4           first_active_month_weekday\n",
       "5             first_active_month_month\n",
       "6        first_active_month_weekofyear\n",
       "7           first_active_month_quarter\n",
       "8    first_active_month_is_month_start\n",
       "9                         elapsed_time\n",
       "10                      days_feature_1\n",
       "11                      days_feature_2\n",
       "12                      days_feature_3\n",
       "13             hist_transactions_count\n",
       "14                 hist_category_1_sum\n",
       "15             hist_category_2_nunique\n",
       "16             hist_category_3_nunique\n",
       "17            hist_merchant_id_nunique\n",
       "18               hist_state_id_nunique\n",
       "19           hist_subsector_id_nunique\n",
       "20                hist_city_id_nunique\n",
       "21   hist_merchant_category_id_nunique\n",
       "22           hist_installments_nunique\n",
       "23             hist_purchase_month_max\n",
       "24             hist_purchase_month_min\n",
       "25                 hist_month_diff_max\n",
       "26                 hist_month_diff_min\n",
       "27             hist_purchase_date_diff\n",
       "28          hist_purchase_date_uptonow\n",
       "29            union_transactions_count\n",
       "30                union_category_1_sum\n",
       "31            union_category_2_nunique\n",
       "32            union_category_3_nunique\n",
       "33           union_merchant_id_nunique\n",
       "34              union_state_id_nunique\n",
       "35          union_subsector_id_nunique\n",
       "36               union_city_id_nunique\n",
       "37  union_merchant_category_id_nunique\n",
       "38          union_installments_nunique"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=int_cols, columns=['ffm_columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = [train[c].max() + 1 for c in int_cols]\n",
    "len(max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in int_cols:\n",
    "    trainno = len(train[c].unique())\n",
    "    testno = len(test[c].unique())\n",
    "    print(c,trainno,testno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train[int_cols].values\n",
    "y = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.insert(1,'target',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = int_cols\n",
    "numerics = []\n",
    "\n",
    "currentcode = len(numerics)\n",
    "catdict = {}\n",
    "catcodes = {}\n",
    "for x in numerics:\n",
    "    catdict[x] = 0\n",
    "for x in categories:\n",
    "    catdict[x] = 1\n",
    "\n",
    "noofrows = train.shape[0]\n",
    "noofcolumns = len(features)\n",
    "with open(\"alltrainffm.txt\", \"w\") as text_file:\n",
    "    for n, r in enumerate(range(noofrows)):\n",
    "        if((n%100000)==0):\n",
    "            print('Row',n)\n",
    "        datastring = \"\"\n",
    "        datarow = train.iloc[r].to_dict()\n",
    "        datastring += str(int(datarow['target']))\n",
    "\n",
    "\n",
    "        for i, x in enumerate(catdict.keys()):\n",
    "            if(catdict[x]==0):\n",
    "                datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x])\n",
    "            else:\n",
    "                if(x not in catcodes):\n",
    "                    catcodes[x] = {}\n",
    "                    currentcode +=1\n",
    "                    catcodes[x][datarow[x]] = currentcode\n",
    "                elif(datarow[x] not in catcodes[x]):\n",
    "                    currentcode +=1\n",
    "                    catcodes[x][datarow[x]] = currentcode\n",
    "\n",
    "                code = catcodes[x][datarow[x]]\n",
    "                datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\"\n",
    "        datastring += '\\n'\n",
    "        text_file.write(datastring)\n",
    "        \n",
    "noofrows = test.shape[0]\n",
    "noofcolumns = len(features)\n",
    "with open(\"alltestffm.txt\", \"w\") as text_file:\n",
    "    for n, r in enumerate(range(noofrows)):\n",
    "        if((n%100000)==0):\n",
    "            print('Row',n)\n",
    "        datastring = \"\"\n",
    "        datarow = test.iloc[r].to_dict()\n",
    "        datastring += str(int(datarow['target']))\n",
    "\n",
    "        for i, x in enumerate(catdict.keys()):\n",
    "            if(catdict[x]==0):\n",
    "                datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x])\n",
    "            else:\n",
    "                if(x not in catcodes):\n",
    "                    catcodes[x] = {}\n",
    "                    currentcode +=1\n",
    "                    catcodes[x][datarow[x]] = currentcode\n",
    "                elif(datarow[x] not in catcodes[x]):\n",
    "                    currentcode +=1\n",
    "                    catcodes[x][datarow[x]] = currentcode\n",
    "\n",
    "                code = catcodes[x][datarow[x]]\n",
    "                datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\"\n",
    "        datastring += '\\n'\n",
    "        text_file.write(datastring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xlearn as xl\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "X = train[int_cols].values\n",
    "y = train['target'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# param:\n",
    "#  0. binary classification\n",
    "#  1. model scale: 0.1\n",
    "#  2. epoch number: 10 (auto early-stop)\n",
    "#  3. learning rate: 0.1\n",
    "#  4. regular lambda: 1.0\n",
    "#  5. use sgd optimization method\n",
    "linear_model = xl.LRModel(task='reg', init=0.1,\n",
    "                          epoch=10, lr=0.1,\n",
    "                          reg_lambda=1.0, opt='sgd', metric='rmse')\n",
    "\n",
    "# Start to train\n",
    "linear_model.fit(X_train, y_train,\n",
    "                 eval_set=[X_val, y_val],\n",
    "                 is_lock_free=False)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = linear_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlearn as xl\n",
    "\n",
    "# Training task\n",
    "ffm_model = xl.create_ffm()                # Use field-aware factorization machine (ffm)\n",
    "ffm_model.setTrain(\"./alltrainffm.txt.txt\")    # Path of training data\n",
    "\n",
    "# param:\n",
    "#  0. task: binary classification\n",
    "#  1. learning rate : 0.2\n",
    "#  2. regular lambda : 0.002\n",
    "param = {'task':'binary', 'lr':0.2, 'lambda':0.002}\n",
    "\n",
    "# Train model\n",
    "ffm_model.fit(param, \"./model.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffm_cols = pd.read_csv('../remove_outlier_py/ffm/ffm_cols.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['feature_1', 'feature_2', 'feature_3', 'first_active_month_year', 'first_active_month_weekday', 'first_active_month_month', 'first_active_month_weekofyear', 'first_active_month_quarter', 'first_active_month_is_month_start', 'elapsed_time', 'days_feature_1', 'days_feature_2', 'days_feature_3', 'hist_transactions_count', 'hist_category_1_sum', 'hist_category_2_nunique', 'hist_category_3_nunique', 'hist_merchant_id_nunique', 'hist_state_id_nunique', 'hist_subsector_id_nunique', 'hist_city_id_nunique', 'hist_merchant_category_id_nunique', 'hist_installments_nunique', 'hist_purchase_month_max', 'hist_purchase_month_min', 'hist_month_diff_max', 'hist_month_diff_min', 'hist_purchase_date_diff', 'hist_purchase_date_uptonow', 'union_transactions_count', 'union_category_1_sum', 'union_category_2_nunique', 'union_category_3_nunique', 'union_merchant_id_nunique', 'union_state_id_nunique', 'union_subsector_id_nunique', 'union_city_id_nunique', 'union_merchant_category_id_nunique', 'union_installments_nunique']"
      ],
      "text/plain": [
       "['feature_1',\n",
       " 'feature_2',\n",
       " 'feature_3',\n",
       " 'first_active_month_year',\n",
       " 'first_active_month_weekday',\n",
       " 'first_active_month_month',\n",
       " 'first_active_month_weekofyear',\n",
       " 'first_active_month_quarter',\n",
       " 'first_active_month_is_month_start',\n",
       " 'elapsed_time',\n",
       " 'days_feature_1',\n",
       " 'days_feature_2',\n",
       " 'days_feature_3',\n",
       " 'hist_transactions_count',\n",
       " 'hist_category_1_sum',\n",
       " 'hist_category_2_nunique',\n",
       " 'hist_category_3_nunique',\n",
       " 'hist_merchant_id_nunique',\n",
       " 'hist_state_id_nunique',\n",
       " 'hist_subsector_id_nunique',\n",
       " 'hist_city_id_nunique',\n",
       " 'hist_merchant_category_id_nunique',\n",
       " 'hist_installments_nunique',\n",
       " 'hist_purchase_month_max',\n",
       " 'hist_purchase_month_min',\n",
       " 'hist_month_diff_max',\n",
       " 'hist_month_diff_min',\n",
       " 'hist_purchase_date_diff',\n",
       " 'hist_purchase_date_uptonow',\n",
       " 'union_transactions_count',\n",
       " 'union_category_1_sum',\n",
       " 'union_category_2_nunique',\n",
       " 'union_category_3_nunique',\n",
       " 'union_merchant_id_nunique',\n",
       " 'union_state_id_nunique',\n",
       " 'union_subsector_id_nunique',\n",
       " 'union_city_id_nunique',\n",
       " 'union_merchant_category_id_nunique',\n",
       " 'union_installments_nunique']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ffm_cols['ffm_cols'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
