{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from datetime import datetime, date\n",
    "from collections import defaultdict\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "import eli5\n",
    "from eli5.lightgbm import explain_weights_lightgbm\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "import shap \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join('..', 'remove_outlier_data')\n",
    "\n",
    "params = {\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 30, \n",
    "    'objective':'regression',\n",
    "    'max_depth': -1,\n",
    "    'learning_rate': 0.01,\n",
    "    'boosting': 'gbdt',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_seed': 11,\n",
    "    'metric': 'rmse',\n",
    "    'lambda_l1': 0.1,\n",
    "    'verbosity': -1,\n",
    "    'nthread': cpu_count(),\n",
    "    'random_state': 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "features +=  [f'f10{i}.pkl' for i in (2, 3)]\n",
    "features += [f'f11{i}_{j}.pkl' for i in (1, 2) \n",
    "                               for j in ('Y', 'N')]\n",
    "features += [f'f12{i}.pkl' for i in (1, 2)]\n",
    "\n",
    "\n",
    "features += [f'f20{i}.pkl' for i in (2, 3)]\n",
    "features += [f'f21{i}_{j}.pkl' for i in (1, 2)\n",
    "                               for j in ('Y', 'N')]\n",
    "\n",
    "features += [f'f40{i}.pkl' for i in (2, 3)]\n",
    "features += [f'f41{i}_{j}.pkl' for i in (1, 2)\n",
    "                               for j in ('Y', 'N')]\n",
    "features += [f'f42{i}.pkl' for i in (1, 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "\n",
    "for f in tqdm(features):\n",
    "    t = pd.read_pickle(os.path.join('..', 'remove_outlier_feature', f))\n",
    "    train = pd.merge(train, t, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train.columns.values\n",
    "for f in [\n",
    "    'new_purchase_date_max', 'new_purchase_date_min',\n",
    "    'hist_purchase_date_max', 'hist_purchase_date_min', \n",
    "    'N_hist_auth_purchase_date_max', 'N_hist_auth_purchase_date_min',\n",
    "    'Y_hist_auth_purchase_date_max', 'Y_hist_auth_purchase_date_min', \n",
    "    'Y_new_auth_purchase_date_max', 'Y_new_auth_purchase_date_min', \n",
    "    'N_new_auth_purchase_date_max', 'N_new_auth_purchase_date_min',\n",
    "    'Y_new_auth_purchase_date_max_x', 'Y_new_auth_purchase_date_min_x', \n",
    "    'N_new_auth_purchase_date_max_x', 'N_new_auth_purchase_date_min_x', \n",
    "    'Y_new_auth_purchase_date_max_y', 'Y_new_auth_purchase_date_min_y', \n",
    "    'N_new_auth_purchase_date_max_y', 'N_new_auth_purchase_date_min_y'\n",
    "]:\n",
    "    if f in cols:\n",
    "        train[f] = train[f].astype(np.int64) * 1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']\n",
    "\n",
    "col_not_to_use = ['first_active_month', 'card_id', 'target']\n",
    "col_to_use = [c for c in train.columns if c not in col_not_to_use]\n",
    "\n",
    "train = train[col_to_use]\n",
    "train['feature_3'] = train['feature_3'].astype(int)\n",
    "\n",
    "categorical_features = ['feature_1', 'feature_2', 'feature_3']\n",
    "\n",
    "for col in categorical_features:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train[col].values.astype('str')))\n",
    "    train[col] = lbl.transform(list(train[col].values.astype('str')))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=6)\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "    dtrain = lgb.Dataset(X.iloc[train_index], label=y.iloc[train_index])\n",
    "    dvalid = lgb.Dataset(X.iloc[valid_index], label=y.iloc[valid_index])\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        20000,          \n",
    "        valid_sets=[dtrain, dvalid],\n",
    "        verbose_eval=200,\n",
    "        early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "shap_values = explainer.shap_values(X.iloc[valid_index])\n",
    "shap.summary_plot(shap_values, X.iloc[valid_index], max_display=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# libFFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "100%|██████████| 22/22 [00:20<00:00,  1.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dot, Reshape, Add, Subtract\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "\n",
    "#==============================================================================\n",
    "PREF = 'f501'\n",
    "\n",
    "KEY = 'card_id'\n",
    "\n",
    "# =============================================================================\n",
    "# def\n",
    "# =============================================================================\n",
    "\n",
    "def get_embed(x_input, x_size, k_latent):\n",
    "    if x_size > 0:  \n",
    "        embed = Embedding(x_size, k_latent, input_length=1,\n",
    "                          embeddings_regularizer=l2(embedding_reg))(x_input)\n",
    "        embed = Flatten()(embed)\n",
    "    else:\n",
    "        embed = Dense(k_latent, kernel_regularizer=l2(embedding_reg))(x_input)\n",
    "    return embed\n",
    "\n",
    "\n",
    "def build_model_1(X, fsize):\n",
    "    dim_input = len(fsize)\n",
    "\n",
    "    input_x = [Input(shape=(1,)) for i in range(dim_input)]\n",
    "\n",
    "    biases = [get_embed(x, size, 1) for (x, size) in zip(input_x, fsize)]\n",
    "\n",
    "    factors = [get_embed(x, size, k_latent)\n",
    "               for (x, size) in zip(input_x, fsize)]\n",
    "\n",
    "    s = Add()(factors)\n",
    "\n",
    "    diffs = [Subtract()([s, x]) for x in factors]\n",
    "\n",
    "    dots = [Dot(axes=1)([d, x]) for d, x in zip(diffs, factors)]\n",
    "\n",
    "    x = Concatenate()(biases + dots)\n",
    "    x = BatchNormalization()(x)\n",
    "    output = Dense(1, activation='relu', kernel_regularizer=l2(kernel_reg))(x)\n",
    "    model = Model(inputs=input_x, outputs=[output])\n",
    "    opt = Adam(clipnorm=0.5)\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    output_f = factors + biases\n",
    "    model_features = Model(inputs=input_x, outputs=output_f)\n",
    "\n",
    "    return model, model_features\n",
    "\n",
    "# =============================================================================\n",
    "# main\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "train = pd.read_csv('../remove_outlier_data/train.csv')\n",
    "test = pd.read_csv('../remove_outlier_data/test.csv')\n",
    "\n",
    "\n",
    "features = []\n",
    "features +=  [f'f10{i}.pkl' for i in (2, 3)]\n",
    "features += [f'f11{i}_{j}.pkl' for i in (1, 2) \n",
    "                               for j in ('Y', 'N')]\n",
    "features += [f'f12{i}.pkl' for i in (1, 2)]\n",
    "\n",
    "\n",
    "features += [f'f20{i}.pkl' for i in (2, 3)]\n",
    "features += [f'f21{i}_{j}.pkl' for i in (1, 2)\n",
    "                               for j in ('Y', 'N')]\n",
    "\n",
    "features += [f'f40{i}.pkl' for i in (2, 3)]\n",
    "features += [f'f41{i}_{j}.pkl' for i in (1, 2)\n",
    "                               for j in ('Y', 'N')]\n",
    "features += [f'f42{i}.pkl' for i in (1, 2)]\n",
    "\n",
    "for f in tqdm(features):\n",
    "    t = pd.read_pickle(os.path.join('..', 'remove_outlier_feature', f))\n",
    "    train = pd.merge(train, t, on='card_id', how='left')\n",
    "    test = pd.merge(test, t, on='card_id', how='left')\n",
    "\n",
    "df = pd.concat([train, test], axis=0, sort=False)\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "      <th>outliers_mean</th>\n",
       "      <th>first_active_month_year</th>\n",
       "      <th>first_active_month_weekday</th>\n",
       "      <th>first_active_month_month</th>\n",
       "      <th>...</th>\n",
       "      <th>cnt_std_4_2017_y</th>\n",
       "      <th>cnt_std_4_2018</th>\n",
       "      <th>cnt_std_5_2017_y</th>\n",
       "      <th>cnt_std_6_2017_y</th>\n",
       "      <th>cnt_std_7_2017_y</th>\n",
       "      <th>cnt_std_8_2017_y</th>\n",
       "      <th>cnt_std_9_2017_y</th>\n",
       "      <th>cnt_std_10_2017_y</th>\n",
       "      <th>cnt_std_11_2017_y</th>\n",
       "      <th>cnt_std_12_2017_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.820283</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.707031</td>\n",
       "      <td>3.558594</td>\n",
       "      <td>2.548828</td>\n",
       "      <td>0.447266</td>\n",
       "      <td>0.514648</td>\n",
       "      <td>0.736816</td>\n",
       "      <td>0.850098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>0.009898</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.97168</td>\n",
       "      <td>1.738281</td>\n",
       "      <td>1.924805</td>\n",
       "      <td>0.577148</td>\n",
       "      <td>0.358643</td>\n",
       "      <td>0.344238</td>\n",
       "      <td>0.887695</td>\n",
       "      <td>2.158203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688056</td>\n",
       "      <td>0.008133</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.828125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142495</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>2017</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316162</td>\n",
       "      <td>1.221680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.159749</td>\n",
       "      <td>0.011128</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.344727</td>\n",
       "      <td>1.065430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 599 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0         2017-06-01  C_ID_92a2005557          5          2          1   \n",
       "1         2017-01-01  C_ID_3d0044924f          4          1          0   \n",
       "2         2016-08-01  C_ID_d639edf6cd          2          2          0   \n",
       "3         2017-09-01  C_ID_186d6a6901          4          3          0   \n",
       "4         2017-11-01  C_ID_cdbd2c0db2          1          3          0   \n",
       "\n",
       "     target  outliers_mean  first_active_month_year  \\\n",
       "0 -0.820283       0.009014                     2017   \n",
       "1  0.392913       0.009898                     2017   \n",
       "2  0.688056       0.008133                     2016   \n",
       "3  0.142495       0.009820                     2017   \n",
       "4 -0.159749       0.011128                     2017   \n",
       "\n",
       "   first_active_month_weekday  first_active_month_month        ...          \\\n",
       "0                           3                         6        ...           \n",
       "1                           6                         1        ...           \n",
       "2                           0                         8        ...           \n",
       "3                           4                         9        ...           \n",
       "4                           2                        11        ...           \n",
       "\n",
       "   cnt_std_4_2017_y  cnt_std_4_2018  cnt_std_5_2017_y  cnt_std_6_2017_y  \\\n",
       "0               0.0             0.0           0.00000          0.707031   \n",
       "1               0.0             0.0           0.97168          1.738281   \n",
       "2               0.0             0.0           0.00000          0.000000   \n",
       "3               0.0             0.0           0.00000          0.000000   \n",
       "4               0.0             0.0           0.00000          0.000000   \n",
       "\n",
       "   cnt_std_7_2017_y  cnt_std_8_2017_y  cnt_std_9_2017_y  cnt_std_10_2017_y  \\\n",
       "0          3.558594          2.548828          0.447266           0.514648   \n",
       "1          1.924805          0.577148          0.358643           0.344238   \n",
       "2          0.000000          0.000000          0.000000           2.828125   \n",
       "3          0.000000          0.000000          0.316162           1.221680   \n",
       "4          0.000000          0.000000          0.000000           0.000000   \n",
       "\n",
       "   cnt_std_11_2017_y  cnt_std_12_2017_y  \n",
       "0           0.736816           0.850098  \n",
       "1           0.887695           2.158203  \n",
       "2           0.000000           0.000000  \n",
       "3           0.000000           0.447266  \n",
       "4           1.344727           1.065430  \n",
       "\n",
       "[5 rows x 599 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['first_active_month', 'card_id', 'feature_1', 'feature_2', 'feature_3',\n",
       "       'target', 'outliers_mean', 'first_active_month_year',\n",
       "       'first_active_month_weekday', 'first_active_month_month',\n",
       "       ...\n",
       "       'cnt_std_4_2017_y', 'cnt_std_4_2018', 'cnt_std_5_2017_y',\n",
       "       'cnt_std_6_2017_y', 'cnt_std_7_2017_y', 'cnt_std_8_2017_y',\n",
       "       'cnt_std_9_2017_y', 'cnt_std_10_2017_y', 'cnt_std_11_2017_y',\n",
       "       'cnt_std_12_2017_y'],\n",
       "      dtype='object', length=599)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int: feature_1\n",
      "int: feature_2\n",
      "int: feature_3\n",
      "int: first_active_month_year\n",
      "int: first_active_month_weekday\n",
      "int: first_active_month_month\n",
      "int: first_active_month_weekofyear\n",
      "int: first_active_month_quarter\n",
      "int: first_active_month_is_month_start\n",
      "int: elapsed_time\n",
      "int: days_feature_1\n",
      "int: days_feature_2\n",
      "int: days_feature_3\n",
      "int: hist_transactions_count\n",
      "int: hist_category_1_sum\n",
      "int: hist_category_2_nunique\n",
      "int: hist_category_3_nunique\n",
      "int: hist_merchant_id_nunique\n",
      "int: hist_state_id_nunique\n",
      "int: hist_subsector_id_nunique\n",
      "int: hist_city_id_nunique\n",
      "int: hist_merchant_category_id_nunique\n",
      "int: hist_installments_nunique\n",
      "int: hist_purchase_month_max\n",
      "int: hist_purchase_month_min\n",
      "int: hist_month_diff_max\n",
      "int: hist_month_diff_min\n",
      "int: hist_purchase_date_diff\n",
      "int: hist_purchase_date_uptonow\n",
      "int: Y_hist_auth_category_1_sum\n",
      "int: Y_hist_auth_category_2_nunique\n",
      "int: Y_hist_auth_category_3_nunique\n",
      "int: Y_hist_auth_merchant_id_nunique\n",
      "int: Y_hist_auth_state_id_nunique\n",
      "int: Y_hist_auth_subsector_id_nunique\n",
      "int: Y_hist_auth_city_id_nunique\n",
      "int: Y_hist_auth_merchant_category_id_nunique\n",
      "int: Y_hist_auth_installments_nunique\n",
      "int: Y_hist_auth_purchase_month_max\n",
      "int: Y_hist_auth_purchase_month_min\n",
      "int: Y_hist_auth_month_diff_max\n",
      "int: Y_hist_auth_month_diff_min\n",
      "int: Y_hist_auth_purchase_date_diff\n",
      "int: Y_hist_auth_purchase_date_uptonow\n",
      "int: union_transactions_count\n",
      "int: union_category_1_sum\n",
      "int: union_category_2_nunique\n",
      "int: union_category_3_nunique\n",
      "int: union_merchant_id_nunique\n",
      "int: union_state_id_nunique\n",
      "int: union_subsector_id_nunique\n",
      "int: union_city_id_nunique\n",
      "int: union_merchant_category_id_nunique\n",
      "int: union_installments_nunique\n",
      "int: Y_union_auth_category_1_sum\n",
      "int: Y_union_auth_category_2_nunique\n",
      "int: Y_union_auth_category_3_nunique\n",
      "int: Y_union_auth_merchant_id_nunique\n",
      "int: Y_union_auth_state_id_nunique\n",
      "int: Y_union_auth_subsector_id_nunique\n",
      "int: Y_union_auth_city_id_nunique\n",
      "int: Y_union_auth_merchant_category_id_nunique\n",
      "int: Y_union_auth_installments_nunique\n"
     ]
    }
   ],
   "source": [
    "for c, d in zip(df.columns, df.dtypes):\n",
    "    if ('int' in str(d)):\n",
    "        print('int:', c)\n",
    "#     if ('float' in str(d)):\n",
    "#         print('float:', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 18\n",
    "np.random.seed(SEED)\n",
    "\n",
    "features = [\n",
    "#     'feature_1', 'feature_2', 'feature_3', \n",
    "#     'days_feature_1', 'days_feature_2', 'days_feature_3',\n",
    "    'hist_category_1_sum', 'hist_category_2_nunique', 'hist_category_3_nunique',\n",
    "    'Y_new_auth_category_1_sum', 'Y_new_auth_category_2_nunique', 'Y_new_auth_category_3_nunique',\n",
    "    'union_category_1_sum', 'union_category_2_nunique', 'union_category_3_nunique',\n",
    "]\n",
    "\n",
    "fsize = [int(df[f].max()) + 1 for f in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 456216 samples, validate on 456216 samples\n",
      "Epoch 1/20\n",
      "456216/456216 [==============================] - 4s 9us/step - loss: 0.5567 - val_loss: 0.5278\n",
      "Epoch 2/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.5176 - val_loss: 0.4921\n",
      "Epoch 3/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.4856 - val_loss: 0.4643\n",
      "Epoch 4/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.4599 - val_loss: 0.4440\n",
      "Epoch 5/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.4399 - val_loss: 0.4293\n",
      "Epoch 6/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.4242 - val_loss: 0.4169\n",
      "Epoch 7/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.4113 - val_loss: 0.4057\n",
      "Epoch 8/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.4000 - val_loss: 0.3949\n",
      "Epoch 9/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.3879 - val_loss: 0.3818\n",
      "Epoch 10/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.3731 - val_loss: 0.3640\n",
      "Epoch 11/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.3557 - val_loss: 0.3433\n",
      "Epoch 12/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.3379 - val_loss: 0.3233\n",
      "Epoch 13/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.3210 - val_loss: 0.3075\n",
      "Epoch 14/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.3064 - val_loss: 0.2978\n",
      "Epoch 15/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.2950 - val_loss: 0.2919\n",
      "Epoch 16/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.2859 - val_loss: 0.2847\n",
      "Epoch 17/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.2772 - val_loss: 0.2767\n",
      "Epoch 18/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.2695 - val_loss: 0.2693\n",
      "Epoch 19/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.2626 - val_loss: 0.2621\n",
      "Epoch 20/20\n",
      "456216/456216 [==============================] - 1s 3us/step - loss: 0.2554 - val_loss: 0.2545\n"
     ]
    }
   ],
   "source": [
    "X = df.groupby(features)['card_id'].count()\n",
    "\n",
    "X = X.unstack().fillna(0)\n",
    "X = X.stack().astype('float32')\n",
    "X = np.log1p(X).reset_index()\n",
    "X.columns = features + ['num']\n",
    "\n",
    "X_train = [X[f].values for f in features]\n",
    "y_train = (X[['num']].values).astype('float32')\n",
    "\n",
    "k_latent = 3\n",
    "embedding_reg = 0.0002\n",
    "kernel_reg = 0.1\n",
    "\n",
    "model, model_features = build_model_1(X_train, fsize)\n",
    "\n",
    "n_epochs = 20\n",
    "\n",
    "batch_size = 2 ** 17\n",
    "model, model_features = build_model_1(X_train, fsize)\n",
    "earlystopper = EarlyStopping(patience=0, verbose=50)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,  y_train,\n",
    "    epochs=n_epochs, batch_size=batch_size, verbose=1, shuffle=True,\n",
    "    validation_data=(X_train, y_train),\n",
    "    callbacks=[earlystopper],\n",
    ")\n",
    "# model.save('weights/{}_weights.h5'.format(str(date.today()).replace('-', '')))\n",
    "\n",
    "X_pred = model_features.predict(X_train, batch_size=batch_size)\n",
    "\n",
    "factors = X_pred[:len(features)]\n",
    "\n",
    "biases = X_pred[len(features):2*len(features)]\n",
    "\n",
    "for f, X_p in zip(features, factors):\n",
    "    for i in range(k_latent):\n",
    "        X['%s_fm_factor_%d' % (f, i)] = X_p[:, i]\n",
    "\n",
    "# for f, X_p in zip(features, biases):\n",
    "#     for i in range(k_latent):\n",
    "#         X['%s_fm_bias' % (f, i)] = X_p[:, 0i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(456216, 37)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(325540, 599)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hist_category_1_sum</th>\n",
       "      <th>hist_category_2_nunique</th>\n",
       "      <th>hist_category_3_nunique</th>\n",
       "      <th>Y_new_auth_category_1_sum</th>\n",
       "      <th>Y_new_auth_category_2_nunique</th>\n",
       "      <th>Y_new_auth_category_3_nunique</th>\n",
       "      <th>union_category_1_sum</th>\n",
       "      <th>union_category_2_nunique</th>\n",
       "      <th>union_category_3_nunique</th>\n",
       "      <th>num</th>\n",
       "      <th>...</th>\n",
       "      <th>Y_new_auth_category_3_nunique_fm_factor_2</th>\n",
       "      <th>union_category_1_sum_fm_factor_0</th>\n",
       "      <th>union_category_1_sum_fm_factor_1</th>\n",
       "      <th>union_category_1_sum_fm_factor_2</th>\n",
       "      <th>union_category_2_nunique_fm_factor_0</th>\n",
       "      <th>union_category_2_nunique_fm_factor_1</th>\n",
       "      <th>union_category_2_nunique_fm_factor_2</th>\n",
       "      <th>union_category_3_nunique_fm_factor_0</th>\n",
       "      <th>union_category_3_nunique_fm_factor_1</th>\n",
       "      <th>union_category_3_nunique_fm_factor_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.332719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070399</td>\n",
       "      <td>0.066835</td>\n",
       "      <td>-0.009863</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.058415</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.043799</td>\n",
       "      <td>0.108552</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>-0.116753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.610918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070399</td>\n",
       "      <td>0.066835</td>\n",
       "      <td>-0.009863</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.058415</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.043799</td>\n",
       "      <td>0.047153</td>\n",
       "      <td>-0.069048</td>\n",
       "      <td>-0.104757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070399</td>\n",
       "      <td>0.066835</td>\n",
       "      <td>-0.009863</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.058415</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.043799</td>\n",
       "      <td>-0.054853</td>\n",
       "      <td>-0.044287</td>\n",
       "      <td>0.122805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070399</td>\n",
       "      <td>0.066835</td>\n",
       "      <td>-0.009863</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.058415</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.043799</td>\n",
       "      <td>-0.052913</td>\n",
       "      <td>0.074434</td>\n",
       "      <td>-0.024833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037986</td>\n",
       "      <td>0.066835</td>\n",
       "      <td>-0.009863</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.058415</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.043799</td>\n",
       "      <td>0.108552</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>-0.116753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hist_category_1_sum  hist_category_2_nunique  hist_category_3_nunique  \\\n",
       "0                    0                        0                        1   \n",
       "1                    0                        0                        1   \n",
       "2                    0                        0                        1   \n",
       "3                    0                        0                        1   \n",
       "4                    0                        0                        1   \n",
       "\n",
       "   Y_new_auth_category_1_sum  Y_new_auth_category_2_nunique  \\\n",
       "0                        0.0                            0.0   \n",
       "1                        0.0                            0.0   \n",
       "2                        0.0                            0.0   \n",
       "3                        0.0                            0.0   \n",
       "4                        0.0                            0.0   \n",
       "\n",
       "   Y_new_auth_category_3_nunique  union_category_1_sum  \\\n",
       "0                            1.0                     0   \n",
       "1                            1.0                     0   \n",
       "2                            1.0                     0   \n",
       "3                            1.0                     0   \n",
       "4                            2.0                     0   \n",
       "\n",
       "   union_category_2_nunique  union_category_3_nunique       num  \\\n",
       "0                         0                         1  5.332719   \n",
       "1                         0                         2  3.610918   \n",
       "2                         0                         3  0.000000   \n",
       "3                         0                         4  0.000000   \n",
       "4                         0                         1  0.000000   \n",
       "\n",
       "                   ...                   \\\n",
       "0                  ...                    \n",
       "1                  ...                    \n",
       "2                  ...                    \n",
       "3                  ...                    \n",
       "4                  ...                    \n",
       "\n",
       "   Y_new_auth_category_3_nunique_fm_factor_2  \\\n",
       "0                                   0.070399   \n",
       "1                                   0.070399   \n",
       "2                                   0.070399   \n",
       "3                                   0.070399   \n",
       "4                                  -0.037986   \n",
       "\n",
       "   union_category_1_sum_fm_factor_0  union_category_1_sum_fm_factor_1  \\\n",
       "0                          0.066835                         -0.009863   \n",
       "1                          0.066835                         -0.009863   \n",
       "2                          0.066835                         -0.009863   \n",
       "3                          0.066835                         -0.009863   \n",
       "4                          0.066835                         -0.009863   \n",
       "\n",
       "   union_category_1_sum_fm_factor_2  union_category_2_nunique_fm_factor_0  \\\n",
       "0                            -0.075                              0.058415   \n",
       "1                            -0.075                              0.058415   \n",
       "2                            -0.075                              0.058415   \n",
       "3                            -0.075                              0.058415   \n",
       "4                            -0.075                              0.058415   \n",
       "\n",
       "   union_category_2_nunique_fm_factor_1  union_category_2_nunique_fm_factor_2  \\\n",
       "0                              0.002382                              0.043799   \n",
       "1                              0.002382                              0.043799   \n",
       "2                              0.002382                              0.043799   \n",
       "3                              0.002382                              0.043799   \n",
       "4                              0.002382                              0.043799   \n",
       "\n",
       "   union_category_3_nunique_fm_factor_0  union_category_3_nunique_fm_factor_1  \\\n",
       "0                              0.108552                              0.005149   \n",
       "1                              0.047153                             -0.069048   \n",
       "2                             -0.054853                             -0.044287   \n",
       "3                             -0.052913                              0.074434   \n",
       "4                              0.108552                              0.005149   \n",
       "\n",
       "   union_category_3_nunique_fm_factor_2  \n",
       "0                             -0.116753  \n",
       "1                             -0.104757  \n",
       "2                              0.122805  \n",
       "3                             -0.024833  \n",
       "4                             -0.116753  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df[['card_id', 'feature_1', 'feature_2', 'feature_3']], X, on=['feature_1', 'feature_2', 'feature_3'], how='left')\n",
    "df = df.drop(features, axis=1)\n",
    "df.to_pickle(f'../remove_outlier_feature/{PREF}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from keras.layers import Input, Embedding, Dense,Flatten, merge,Activation\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2 as l2_reg\n",
    "from keras import initializers\n",
    "import itertools\n",
    "\n",
    "\n",
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.ceil(size/float(batch_size)))\n",
    "    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n",
    "\n",
    "\n",
    "def batch_generator(X,y,batch_size=128,shuffle=True):\n",
    "    sample_size = X[0].shape[0]\n",
    "    index_array = np.arange(sample_size)\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(index_array)\n",
    "        batches = make_batches(sample_size, batch_size)\n",
    "        for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "            batch_ids = index_array[batch_start:batch_end]\n",
    "            X_batch = [X[i][batch_ids] for i in range(len(X))]\n",
    "            y_batch = y[batch_ids]\n",
    "            yield X_batch,y_batch\n",
    "\n",
    "\n",
    "def test_batch_generator(X,y,batch_size=128):\n",
    "    sample_size = X[0].shape[0]\n",
    "    index_array = np.arange(sample_size)\n",
    "    batches = make_batches(sample_size, batch_size)\n",
    "    for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "        batch_ids = index_array[batch_start:batch_end]\n",
    "        X_batch = [X[i][batch_ids] for i in range(len(X))]\n",
    "        y_batch = y[batch_ids]\n",
    "        yield X_batch,y_batch\n",
    "\n",
    "\n",
    "def predict_batch(model,X_t,batch_size=128):\n",
    "    outcome = []\n",
    "    for X_batch,y_batch in test_batch_generator(X_t,np.zeros(X_t[0].shape[0]),batch_size=batch_size):\n",
    "        outcome.append(model.predict(X_batch,batch_size=batch_size))\n",
    "    outcome = np.concatenate(outcome).ravel()\n",
    "    return outcome\n",
    "\n",
    "\n",
    "\n",
    "def build_model(max_features,K=8,solver='adam',l2=0.0,l2_fm = 0.0):\n",
    "\n",
    "    inputs = []\n",
    "    flatten_layers=[]\n",
    "    columns = range(len(max_features))\n",
    "    for c in columns:\n",
    "        inputs_c = Input(shape=(1,), dtype='int32',name = 'input_%s'%c)\n",
    "        num_c = max_features[c]\n",
    "\n",
    "        embed_c = Embedding(\n",
    "                        num_c,\n",
    "                        K,\n",
    "                        input_length=1,\n",
    "                        name = 'embed_%s'%c,\n",
    "                        W_regularizer=l2_reg(l2_fm)\n",
    "                        )(inputs_c)\n",
    "\n",
    "        flatten_c = Flatten()(embed_c)\n",
    "        inputs.append(inputs_c)\n",
    "        flatten_layers.append(flatten_c)\n",
    "\n",
    "    fm_layers = []\n",
    "    for emb1,emb2 in itertools.combinations(flatten_layers, 2):\n",
    "        dot_layer = merge([emb1,emb2],mode='dot',dot_axes=1)\n",
    "        fm_layers.append(dot_layer)\n",
    "\n",
    "    for c in columns:\n",
    "        num_c = max_features[c]\n",
    "        embed_c = Embedding(\n",
    "                        num_c,\n",
    "                        1,\n",
    "                        input_length=1,\n",
    "                        name = 'linear_%s'%c,\n",
    "                        W_regularizer=l2_reg(l2)\n",
    "                        )(inputs[c])\n",
    "\n",
    "        flatten_c = Flatten()(embed_c)\n",
    "\n",
    "        fm_layers.append(flatten_c)\n",
    "        \n",
    "        \n",
    "    flatten = merge(fm_layers,mode='sum')\n",
    "    outputs = Activation('sigmoid',name='outputs')(flatten)\n",
    "    \n",
    "    model = Model(input=inputs, output=outputs)\n",
    "\n",
    "    model.compile(\n",
    "                optimizer=solver,\n",
    "                loss= 'binary_crossentropy'\n",
    "              )\n",
    "\n",
    "    return model\n",
    "\n",
    "class KerasFM(BaseEstimator):\n",
    "    def __init__(self,max_features=[],K=8,solver='adam',l2=0.0,l2_fm = 0.0):\n",
    "        self.model = build_model(max_features,K,solver,l2=l2,l2_fm = l2_fm)\n",
    "\n",
    "    def fit(self,X,y,batch_size=128,nb_epoch=10,shuffle=True,verbose=1,validation_data=None):\n",
    "        self.model.fit(X,y,batch_size=batch_size,nb_epoch=nb_epoch,shuffle=shuffle,verbose=verbose,validation_data=None)\n",
    "\n",
    "    def fit_generator(self,X,y,batch_size=128,nb_epoch=10,shuffle=True,verbose=1,validation_data=None,callbacks=None):\n",
    "        tr_gen = batch_generator(X,y,batch_size=batch_size,shuffle=shuffle)\n",
    "        if validation_data:\n",
    "            X_test,y_test = validation_data\n",
    "            te_gen = batch_generator(X_test,y_test,batch_size=batch_size,shuffle=False)\n",
    "            nb_val_samples = X_test[-1].shape[0]\n",
    "        else:\n",
    "            te_gen = None\n",
    "            nb_val_samples = None\n",
    "\n",
    "        self.model.fit_generator(\n",
    "                tr_gen, \n",
    "                samples_per_epoch=X[-1].shape[0], \n",
    "                nb_epoch=nb_epoch, \n",
    "                verbose=verbose, \n",
    "                callbacks=callbacks, \n",
    "                validation_data=te_gen, \n",
    "                nb_val_samples=nb_val_samples, \n",
    "                max_q_size=10\n",
    "                )\n",
    "\n",
    "    def predict(self,X,batch_size=128):\n",
    "        y_preds = predict_batch(self.model,X,batch_size=batch_size)\n",
    "        return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join('..', 'remove_outlier_data')\n",
    "\n",
    "KEY = 'card_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "features +=  [f'f10{i}.pkl' for i in (2, 3)]\n",
    "# features += [f'f11{i}_{j}.pkl' for i in (1, 2) \n",
    "#                                for j in ('Y', 'N')]\n",
    "# features += [f'f12{i}.pkl' for i in (1, 2)]\n",
    "\n",
    "\n",
    "features += [f'f20{i}.pkl' for i in (2, 3)]\n",
    "# features += [f'f21{i}_{j}.pkl' for i in (1, 2)\n",
    "#                                for j in ('Y', 'N')]\n",
    "\n",
    "features += [f'f40{i}.pkl' for i in (2, 3)]\n",
    "# features += [f'f41{i}_{j}.pkl' for i in (1, 2)\n",
    "#                                for j in ('Y', 'N')]\n",
    "# features += [f'f42{i}.pkl' for i in (1, 2)]\n",
    "\n",
    "\n",
    "# features = os.listdir('../remove_outlier_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:03<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(PATH, 'test.csv'))\n",
    "\n",
    "for f in tqdm(features):\n",
    "    # print(f'Merge: {f}', end=' ')\n",
    "    t = pd.read_pickle(os.path.join('..', 'remove_outlier_feature', f))\n",
    "    train = pd.merge(train, t, on=KEY, how='left')\n",
    "    test = pd.merge(test, t, on=KEY, how='left')\n",
    "    # print('Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train.columns.values\n",
    "for f in [\n",
    "    'new_purchase_date_max', 'new_purchase_date_min',\n",
    "    'hist_purchase_date_max', 'hist_purchase_date_min', \n",
    "    'N_hist_auth_purchase_date_max', 'N_hist_auth_purchase_date_min',\n",
    "    'Y_hist_auth_purchase_date_max', 'Y_hist_auth_purchase_date_min', \n",
    "    'Y_new_auth_purchase_date_max', 'Y_new_auth_purchase_date_min', \n",
    "    'N_new_auth_purchase_date_max', 'N_new_auth_purchase_date_min',\n",
    "    'Y_new_auth_purchase_date_max_x', 'Y_new_auth_purchase_date_min_x', \n",
    "    'N_new_auth_purchase_date_max_x', 'N_new_auth_purchase_date_min_x', \n",
    "    'Y_new_auth_purchase_date_max_y', 'Y_new_auth_purchase_date_min_y', \n",
    "    'N_new_auth_purchase_date_max_y', 'N_new_auth_purchase_date_min_y'\n",
    "]:\n",
    "    if f in cols:\n",
    "        train[f] = train[f].astype(np.int64) * 1e-9\n",
    "        test[f] = test[f].astype(np.int64) * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_cols = [c for d, c in zip(train.dtypes, train.columns) if str(d).startswith('int')]\n",
    "len(int_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = [train[c].max() + 1 for c in int_cols]\n",
    "len(max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_1 5 5\n",
      "feature_2 3 3\n",
      "feature_3 2 2\n",
      "first_active_month_year 8 8\n",
      "first_active_month_weekday 7 7\n",
      "first_active_month_month 12 12\n",
      "first_active_month_weekofyear 21 21\n",
      "first_active_month_quarter 4 4\n",
      "first_active_month_is_month_start 1 1\n",
      "elapsed_time 75 75\n",
      "days_feature_1 224 223\n",
      "days_feature_2 177 171\n",
      "days_feature_3 75 76\n",
      "hist_transactions_count 1023 943\n",
      "hist_category_1_sum 1014 922\n",
      "hist_category_2_nunique 6 6\n",
      "hist_category_3_nunique 4 4\n",
      "hist_merchant_id_nunique 313 296\n",
      "hist_state_id_nunique 20 19\n",
      "hist_subsector_id_nunique 34 34\n",
      "hist_city_id_nunique 58 49\n",
      "hist_merchant_category_id_nunique 92 87\n",
      "hist_installments_nunique 13 13\n",
      "hist_purchase_month_max 11 11\n",
      "hist_purchase_month_min 11 11\n",
      "hist_month_diff_max 14 14\n",
      "hist_month_diff_min 14 14\n",
      "hist_purchase_date_diff 422 420\n",
      "hist_purchase_date_uptonow 391 388\n",
      "union_transactions_count 1039 941\n",
      "union_category_1_sum 1018 926\n",
      "union_category_2_nunique 6 6\n",
      "union_category_3_nunique 4 4\n",
      "union_merchant_id_nunique 344 327\n",
      "union_state_id_nunique 22 20\n",
      "union_subsector_id_nunique 34 34\n",
      "union_city_id_nunique 61 55\n",
      "union_merchant_category_id_nunique 96 89\n",
      "union_installments_nunique 13 13\n"
     ]
    }
   ],
   "source": [
    "for c in int_cols:\n",
    "    trainno = len(train[c].unique())\n",
    "    testno = len(test[c].unique())\n",
    "    print(c,trainno,testno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train[int_cols].values\n",
    "y = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.insert(1,'target',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>target</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>outliers_mean</th>\n",
       "      <th>first_active_month_year</th>\n",
       "      <th>first_active_month_weekday</th>\n",
       "      <th>first_active_month_month</th>\n",
       "      <th>...</th>\n",
       "      <th>union_purchase_amount_mean_mean</th>\n",
       "      <th>union_purchase_amount_mean_std</th>\n",
       "      <th>union_purchase_amount_std_mean</th>\n",
       "      <th>union_purchase_amount_std_std</th>\n",
       "      <th>union_installments_mean_mean</th>\n",
       "      <th>union_installments_mean_std</th>\n",
       "      <th>union_installments_sum_mean</th>\n",
       "      <th>union_installments_sum_std</th>\n",
       "      <th>union_installments_std_mean</th>\n",
       "      <th>union_installments_std_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>0</td>\n",
       "      <td>C_ID_0ab67a22ab</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016459</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.602051</td>\n",
       "      <td>0.103577</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.126709</td>\n",
       "      <td>1.841797</td>\n",
       "      <td>0.877441</td>\n",
       "      <td>14.601562</td>\n",
       "      <td>12.820312</td>\n",
       "      <td>1.272461</td>\n",
       "      <td>1.043945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>C_ID_130fd0cbdd</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009522</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.640137</td>\n",
       "      <td>0.026550</td>\n",
       "      <td>0.109070</td>\n",
       "      <td>0.058075</td>\n",
       "      <td>1.101562</td>\n",
       "      <td>0.176270</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>7.667969</td>\n",
       "      <td>0.278564</td>\n",
       "      <td>0.402588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>0</td>\n",
       "      <td>C_ID_b709037bc5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015883</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197144</td>\n",
       "      <td>0.851562</td>\n",
       "      <td>0.644531</td>\n",
       "      <td>0.396973</td>\n",
       "      <td>4.785156</td>\n",
       "      <td>2.705078</td>\n",
       "      <td>8.289062</td>\n",
       "      <td>5.964844</td>\n",
       "      <td>2.376953</td>\n",
       "      <td>2.761719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>C_ID_d27d835a9f</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013387</td>\n",
       "      <td>2017</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.570801</td>\n",
       "      <td>0.051727</td>\n",
       "      <td>0.105774</td>\n",
       "      <td>0.048340</td>\n",
       "      <td>2.199219</td>\n",
       "      <td>1.477539</td>\n",
       "      <td>13.796875</td>\n",
       "      <td>10.132812</td>\n",
       "      <td>2.347656</td>\n",
       "      <td>2.720703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>C_ID_2b5e3df5c2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015883</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>1.090820</td>\n",
       "      <td>2.490234</td>\n",
       "      <td>2.353516</td>\n",
       "      <td>2.533203</td>\n",
       "      <td>1.086914</td>\n",
       "      <td>0.149658</td>\n",
       "      <td>8.398438</td>\n",
       "      <td>6.726562</td>\n",
       "      <td>0.212646</td>\n",
       "      <td>0.322021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month  target          card_id  feature_1  feature_2  \\\n",
       "0         2017-04-01       0  C_ID_0ab67a22ab          3          3   \n",
       "1         2017-01-01       0  C_ID_130fd0cbdd          2          3   \n",
       "2         2017-08-01       0  C_ID_b709037bc5          5          1   \n",
       "3         2017-12-01       0  C_ID_d27d835a9f          2          1   \n",
       "4         2015-12-01       0  C_ID_2b5e3df5c2          5          1   \n",
       "\n",
       "   feature_3  outliers_mean  first_active_month_year  \\\n",
       "0          1       0.016459                     2017   \n",
       "1          0       0.009522                     2017   \n",
       "2          1       0.015883                     2017   \n",
       "3          0       0.013387                     2017   \n",
       "4          1       0.015883                     2015   \n",
       "\n",
       "   first_active_month_weekday  first_active_month_month  \\\n",
       "0                           5                         4   \n",
       "1                           6                         1   \n",
       "2                           1                         8   \n",
       "3                           4                        12   \n",
       "4                           1                        12   \n",
       "\n",
       "              ...              union_purchase_amount_mean_mean  \\\n",
       "0             ...                                    -0.602051   \n",
       "1             ...                                    -0.640137   \n",
       "2             ...                                     0.197144   \n",
       "3             ...                                    -0.570801   \n",
       "4             ...                                     1.090820   \n",
       "\n",
       "   union_purchase_amount_mean_std  union_purchase_amount_std_mean  \\\n",
       "0                        0.103577                        0.151367   \n",
       "1                        0.026550                        0.109070   \n",
       "2                        0.851562                        0.644531   \n",
       "3                        0.051727                        0.105774   \n",
       "4                        2.490234                        2.353516   \n",
       "\n",
       "   union_purchase_amount_std_std  union_installments_mean_mean  \\\n",
       "0                       0.126709                      1.841797   \n",
       "1                       0.058075                      1.101562   \n",
       "2                       0.396973                      4.785156   \n",
       "3                       0.048340                      2.199219   \n",
       "4                       2.533203                      1.086914   \n",
       "\n",
       "   union_installments_mean_std  union_installments_sum_mean  \\\n",
       "0                     0.877441                    14.601562   \n",
       "1                     0.176270                    11.750000   \n",
       "2                     2.705078                     8.289062   \n",
       "3                     1.477539                    13.796875   \n",
       "4                     0.149658                     8.398438   \n",
       "\n",
       "   union_installments_sum_std  union_installments_std_mean  \\\n",
       "0                   12.820312                     1.272461   \n",
       "1                    7.667969                     0.278564   \n",
       "2                    5.964844                     2.376953   \n",
       "3                   10.132812                     2.347656   \n",
       "4                    6.726562                     0.212646   \n",
       "\n",
       "   union_installments_std_std  \n",
       "0                    1.043945  \n",
       "1                    0.402588  \n",
       "2                    2.761719  \n",
       "3                    2.720703  \n",
       "4                    0.322021  \n",
       "\n",
       "[5 rows x 143 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0\n",
      "Row 100000\n",
      "Row 200000\n",
      "Row 0\n",
      "Row 100000\n"
     ]
    }
   ],
   "source": [
    "categories = int_cols\n",
    "numerics = []\n",
    "\n",
    "currentcode = len(numerics)\n",
    "catdict = {}\n",
    "catcodes = {}\n",
    "for x in numerics:\n",
    "    catdict[x] = 0\n",
    "for x in categories:\n",
    "    catdict[x] = 1\n",
    "\n",
    "noofrows = train.shape[0]\n",
    "noofcolumns = len(features)\n",
    "with open(\"alltrainffm.txt\", \"w\") as text_file:\n",
    "    for n, r in enumerate(range(noofrows)):\n",
    "        if((n%100000)==0):\n",
    "            print('Row',n)\n",
    "        datastring = \"\"\n",
    "        datarow = train.iloc[r].to_dict()\n",
    "        datastring += str(int(datarow['target']))\n",
    "\n",
    "\n",
    "        for i, x in enumerate(catdict.keys()):\n",
    "            if(catdict[x]==0):\n",
    "                datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x])\n",
    "            else:\n",
    "                if(x not in catcodes):\n",
    "                    catcodes[x] = {}\n",
    "                    currentcode +=1\n",
    "                    catcodes[x][datarow[x]] = currentcode\n",
    "                elif(datarow[x] not in catcodes[x]):\n",
    "                    currentcode +=1\n",
    "                    catcodes[x][datarow[x]] = currentcode\n",
    "\n",
    "                code = catcodes[x][datarow[x]]\n",
    "                datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\"\n",
    "        datastring += '\\n'\n",
    "        text_file.write(datastring)\n",
    "        \n",
    "noofrows = test.shape[0]\n",
    "noofcolumns = len(features)\n",
    "with open(\"alltestffm.txt\", \"w\") as text_file:\n",
    "    for n, r in enumerate(range(noofrows)):\n",
    "        if((n%100000)==0):\n",
    "            print('Row',n)\n",
    "        datastring = \"\"\n",
    "        datarow = test.iloc[r].to_dict()\n",
    "        datastring += str(int(datarow['target']))\n",
    "\n",
    "        for i, x in enumerate(catdict.keys()):\n",
    "            if(catdict[x]==0):\n",
    "                datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x])\n",
    "            else:\n",
    "                if(x not in catcodes):\n",
    "                    catcodes[x] = {}\n",
    "                    currentcode +=1\n",
    "                    catcodes[x][datarow[x]] = currentcode\n",
    "                elif(datarow[x] not in catcodes[x]):\n",
    "                    currentcode +=1\n",
    "                    catcodes[x][datarow[x]] = currentcode\n",
    "\n",
    "                code = catcodes[x][datarow[x]]\n",
    "                datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\"\n",
    "        datastring += '\\n'\n",
    "        text_file.write(datastring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xlearn as xl\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "X = train[int_cols].values\n",
    "y = train['target'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# param:\n",
    "#  0. binary classification\n",
    "#  1. model scale: 0.1\n",
    "#  2. epoch number: 10 (auto early-stop)\n",
    "#  3. learning rate: 0.1\n",
    "#  4. regular lambda: 1.0\n",
    "#  5. use sgd optimization method\n",
    "linear_model = xl.LRModel(task='reg', init=0.1,\n",
    "                          epoch=10, lr=0.1,\n",
    "                          reg_lambda=1.0, opt='sgd', metric='rmse')\n",
    "\n",
    "# Start to train\n",
    "linear_model.fit(X_train, y_train,\n",
    "                 eval_set=[X_val, y_val],\n",
    "                 is_lock_free=False)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = linear_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ..., nan, nan, nan])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlearn as xl\n",
    "\n",
    "# Training task\n",
    "ffm_model = xl.create_ffm()                # Use field-aware factorization machine (ffm)\n",
    "ffm_model.setTrain(\"./alltrainffm.txt.txt\")    # Path of training data\n",
    "\n",
    "# param:\n",
    "#  0. task: binary classification\n",
    "#  1. learning rate : 0.2\n",
    "#  2. regular lambda : 0.002\n",
    "param = {'task':'binary', 'lr':0.2, 'lambda':0.002}\n",
    "\n",
    "# Train model\n",
    "ffm_model.fit(param, \"./model.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
